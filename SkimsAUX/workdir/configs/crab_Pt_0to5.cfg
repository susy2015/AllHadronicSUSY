[CRAB]

jobtype = cmssw
#
#--- scheduler
# caf: CERN-CAF, condor: LPC-CAF, glite: GRID
#scheduler = glite
scheduler = condor
### NOTE: just setting the name of the server (pi, lnl etc etc )
###       crab will submit the jobs to the server...
#
#--- server_name
# comment-out: CERN-CAF, LPC-CAF, cern: GRID
#server_name = cern
#use_server = 0

[CMSSW]

#lumi_mask=Cert_160404-162917_7TeV_PromptReco_Collisions11_JSON.txt
#total_number_of_lumis = -1
#lumis_per_job = 10

#show_prod=1

### The data you want to access (to be found on DBS)
dbs_url = https://cmsdbsprod.cern.ch:8443/cms_dbs_ph_analysis_01_writer/servlet/DBSServlet
datasetpath = /QCD_Pt_0to5_TuneZ2_7TeV_pythia6/lhx-QCD_Pt_0to5_TuneZ2_pythia6_Spring11_ra2Cleaned_Inclusive3Jets_HT300-33d90992b6a26befe289e6bd5bcf20fb/USER
use_parent = 0

#runselection = 160431-160578

#debug_wrapper=1

### The ParameterSet you want to use
###########################
pset = analysisStage_COMB_onSkims_OLDecalFilters_cfg.py

### Splitting parameters
total_number_of_events = -1 
number_of_jobs = 1
###########################

pycfg_params = GlobalTag=START311_V2::All hltName=REDIGI311X mcInfo=1

### The output files (comma separated list)
###########################
output_file = stage0_Zinv_TTbar.root,stage1_Zinv_ANA.root,stage1_Zinv_QCD.root,stage1_Zinv_TTbar.root,stage2_Zinv_ANA.root,stage2_Zinv_QCD.root,stage2_Zinv_TTbar.root,stage3_Zinv_ANA.root,stage3_Zinv_QCD.root,stage3_Zinv_TTbar.root,stage4_Zinv_ANA.root,stage4_Zinv_QCD.root,stage4_Zinv_TTbar.root,stage5_HT500_Zinv_ANA.root,stage5_HT500_Zinv_QCD.root,stage5_HT500_Zinv_TTbar.root,stage5_MHT250_Zinv_ANA.root,stage5_MHT250_Zinv_QCD.root,stage5_MHT250_Zinv_TTbar.root,stage5_Zinv_ANA.root,stage5_Zinv_QCD.root,stage5_Zinv_TTbar.root,stage6_Zinv_ANA.root,stage6_Zinv_QCD.root,stage6_Zinv_TTbar.root,stage7_HT500_Zinv_ANA.root,stage7_HT500_Zinv_QCD.root,stage7_HT500_Zinv_TTbar.root,stage7_MHT250_Zinv_ANA.root,stage7_MHT250_Zinv_QCD.root,stage7_MHT250_Zinv_TTbar.root,stage0_Zinv_stdRA2.root,stage1_Zinv_stdRA2.root,stage2_Zinv_stdRA2.root,stage3_Zinv_stdRA2.root,stage4_Zinv_stdRA2.root,stage5_Zinv_stdRA2.root,stage5_MHT250_Zinv_stdRA2.root,stage5_HT500_Zinv_stdRA2.root,stage6_Zinv_stdRA2.root,stage7_MHT250_Zinv_stdRA2.root,stage7_HT500_Zinv_stdRA2.root,stage4_BEonly_Zinv_stdRA2.root,stage4_TPonly_Zinv_stdRA2.root,stage4_BEandTP_Zinv_stdRA2.root,stage5_BEonly_Zinv_stdRA2.root,stage5_TPonly_Zinv_stdRA2.root,stage5_BEandTP_Zinv_stdRA2.root
#get_edm_output = 1
#output_file = rereco_ECALRecovery.root
###########################

[USER]

### OUTPUT files Management
##  output back into UI
return_data = 1

### OUTPUT files INTO A SE
copy_data = 0

###email notifications
thresholdLevel = 100

#additional_input_files = muonIsoRecoEff*.root, effBtag.root

#storage_element        = cmssrm.fnal.gov
#storage_path           = /srm/managerv2?SFN=/resilient/bues90/
#user_remote_dir = /QCDFlat_Pt500to1000-madgraph_Spring10_v1.4/

#publish!!
#storage_element = cmssrm.fnal.gov
#storage_port = 8443
#storage_path = /srm/managerv2?SFN=/11
#user_remote_dir = /store/user/lhx/2011RA2/May10/
#publish_data = 1
#publish_data_name = QCD_Pt_0to5_TuneZ2_pythia6_Spring11_ra2Cleaned_Inclusive3Jets_HT300
#dbs_url_for_publication = https://cmsdbsprod.cern.ch:8443/cms_dbs_ph_analysis_01_writer/servlet/DBSServlet
#check_user_remote_dir = 0

[GRID]

#remove_default_blacklist=1
#role=t1access

## RB/WMS management:
#rb = CERN
#proxy_server = myproxy.cern.ch

##  Black and White Lists management:
## By Storage
##se_black_list = T0,T1

#se_white_list = grid-srm.physik.rwth-aachen.de

## By ComputingElement
#ce_black_list = cmssrm.fnal.gov, cmsdca.fnal.gov
#ce_black_list = srm.ciemat.es,srm-3.t2.ucsd.edu,hephyse.oeaw.ac.at,maite.iihe.ac.be,t2-srm-02.lnl.infn.it,sbgse1.in2p3.fr,cmssrm.hep.wisc.edu,cmsdcache.pi.infn.it,srm.minnesota.edu,storm.ifca.es
#ce_white_list = storm-fe-cms.cr.cnaf.infn.it
#ce_white_list = srmcms.pic.es

[CONDORG]

# Set this to condor to override the batchsystem defined in gridcat.
#batchsystem = condor

# Specify addition condor_g requirments
# use this requirment to run on a cms dedicated hardare
# globus_rsl = (condor_submit=(requirements 'ClusterName == \"CMS\" && (Arch == \"INTEL\" || Arch == \"X86_64\")'))
# use this requirement to run on the new hardware
#globus_rsl = (condor_submit=(requirements 'regexp(\"cms-*\",Machine)'))
